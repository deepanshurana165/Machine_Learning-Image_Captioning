{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from os import listdir\n",
    "from pickle import dump, load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Photo Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About VGG16 model:\n",
    "\n",
    "\n",
    "Application:\n",
    "\n",
    "1. Given image → find object name in the image.\n",
    "\n",
    "2. It can detect any one of 1000 images.\n",
    "\n",
    "3. It takes input image of size 224 * 244 * 3 (RGB image).\n",
    "\n",
    "\n",
    "Built using:\n",
    "\n",
    "1. Convolutions layers (used only 3*3 size ).\n",
    "\n",
    "2. Max pooling layers (used only 2*2 size).\n",
    "\n",
    "3. Fully connected layers at end.\n",
    "\n",
    "4. Total 16 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_photo_features(directory):\n",
    "    #preparing VGG16 model\n",
    "    model = VGG16()\n",
    "    #removing the last layer from the model since we require only the features and not the classification\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    #summarizing the model\n",
    "    print(model.summary())\n",
    "    #storing the features in a dictionary\n",
    "    photo_features = dict()\n",
    "    for name in listdir(directory):\n",
    "        image_name = directory + '/' + name\n",
    "        #loading the image and converting it to 224 X 224 size\n",
    "        image = load_img(image_name, target_size=(224,224))\n",
    "        #converting image to a 3D numpy array\n",
    "        image = img_to_array(image)\n",
    "        #reshaping data for the model\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "        #preprocessing the images for the model\n",
    "        #The preprocess_input function is meant to adequate your image to the format the model requires.\n",
    "        #Some models use images with values ranging from 0 to 1. Others from -1 to +1.\n",
    "        #Others use the \"caffe\" style, that is not normalized, but is centered.\n",
    "        image = preprocess_input(image)\n",
    "        #getting feature of a single image\n",
    "        photo_feature = model.predict(image, verbose=0)\n",
    "        #getting the image name\n",
    "        image_id = name.split('.')[0]\n",
    "        #storing the feature corresponding to the image in the dictionary\n",
    "        photo_features[image_id] = photo_feature\n",
    "        \n",
    "        print(name)\n",
    "    return photo_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "425088533_a460dc4617.jpg\n",
      "2038662925_f4fa8c2534.jpg\n",
      "2715035273_8fc8b1291c.jpg\n",
      "3615730936_23457575e9.jpg\n",
      "3051125715_db76cebd1e.jpg\n",
      "2899276965_a20b839cfd.jpg\n",
      "909191414_1cf5d85821.jpg\n",
      "3458211052_bb73084398.jpg\n",
      "751074141_feafc7b16c.jpg\n",
      "3682038869_585075b5ff.jpg\n",
      "2375924666_fee50f1cba.jpg\n",
      "481887827_f8975dabf1.jpg\n",
      "497122685_a51b29dc46.jpg\n",
      "3385593926_d3e9c21170.jpg\n",
      "3168841415_c0705a327a.jpg\n",
      "3090593241_93a975fe2b.jpg\n",
      "3549140234_e99b07c739.jpg\n",
      "104136873_5b5d41be75.jpg\n",
      "3338217927_3c5cf3f7c6.jpg\n",
      "2128119486_4407061c40.jpg\n",
      "3146630574_05d9ebbed1.jpg\n",
      "2689358407_9932f1b20c.jpg\n",
      "3352199368_b35f25793e.jpg\n",
      "3553374585_25b1bd6970.jpg\n",
      "10815824_2997e03d76.jpg\n",
      "854333409_38bc1da9dc.jpg\n",
      "439916996_1ddb9dc8e7.jpg\n",
      "2502856739_490db7a657.jpg\n",
      "544122267_e9e0100bc5.jpg\n",
      "2353102255_67d9d2e40a.jpg\n",
      "3632047678_f202609e50.jpg\n",
      "3557316485_574a5f7a89.jpg\n",
      "3515358125_9e1d796244.jpg\n",
      "3049770416_0fb1954315.jpg\n",
      "2245618207_fa486ba2b7.jpg\n",
      "3679407035_708774de34.jpg\n",
      "269986132_91b71e8aaa.jpg\n",
      "2930622766_fa8f84deb1.jpg\n",
      "430173345_86388d8822.jpg\n",
      "140377584_12bdbdf2f8.jpg\n",
      "2579899436_5086a33c7a.jpg\n",
      "2933912528_52b05f84a1.jpg\n",
      "475042270_719ebe6c48.jpg\n",
      "3684680947_f1c460242f.jpg\n",
      "106490881_5a2dd9b7bd.jpg\n",
      "367964525_b1528ac6e4.jpg\n",
      "1884727806_d84f209868.jpg\n",
      "2522297487_57edf117f7.jpg\n",
      "2277081067_d2b4c98bce.jpg\n",
      "236144859_52f9e38885.jpg\n",
      "2971431335_e192613db4.jpg\n",
      "3634785801_4b23184a06.jpg\n",
      "190638179_be9da86589.jpg\n",
      "3439982121_0afc6d5973.jpg\n",
      "3606084228_6286a52875.jpg\n",
      "3218889785_86cb64014f.jpg\n",
      "368393384_86defdcde8.jpg\n",
      "2203449950_e51d0f9065.jpg\n",
      "2860372882_e0ef4131d4.jpg\n",
      "235065283_1f9a3c79db.jpg\n",
      "3672109677_8caa992671.jpg\n",
      "3271468462_701eb88d3b.jpg\n",
      "242324909_06d5a6c44b.jpg\n",
      "2147199188_d2d70b88ec.jpg\n",
      "437404867_209625774d.jpg\n",
      "1355935187_2c99648138.jpg\n",
      "2171891283_dedd9cf416.jpg\n",
      "3504940491_94c43792ed.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c52375d05477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Flicker8k_Dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_photo_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracted Features: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dump(features, open('features.pkl', 'wb'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bdb1c879653c>\u001b[0m in \u001b[0;36mextract_photo_features\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#getting feature of a single image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mphoto_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m#getting the image name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1170\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the directory we are using\n",
    "directory = 'Flicker8k_Dataset'\n",
    "#calling the function\n",
    "features = extract_photo_features(directory)\n",
    "print('Extracted Features: ', len(features))\n",
    "\n",
    "#dumping the features in a pickle file for further use\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load the file containing all the description\n",
    "def load_description_file(filename):\n",
    "    #opening the description file in read mode\n",
    "    file = open(filename, 'r')\n",
    "    #reading the file and storing the descriptions\n",
    "    descriptions = file.read()\n",
    "    #closing the file\n",
    "    file.close()\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "#function to return a dictionary of photo identifiers to the descriptions\n",
    "def photo_to_description_mapping(descriptions):\n",
    "    #dictionary to store the mapping of photo identifiers to descriptions\n",
    "    description_mapping = dict()\n",
    "    #iterating through each line of the descriptions\n",
    "    for line in descriptions.split('\\n'):\n",
    "        #splitting the lines by white space\n",
    "        words = line.split()\n",
    "        #skipping the lines with length less than 2\n",
    "        if len(line)<2:\n",
    "            continue\n",
    "        #the first word is the image_id and the rest are the part of the description of that image\n",
    "        image_id, image_description = words[0], words[1:]\n",
    "        #retaining only the name of the image and removing the extension from it\n",
    "        image_id = image_id.split('.')[0]\n",
    "        #image_descriptions contains comma separated words of the description, hence, converting it back to string\n",
    "        image_description = ' '.join(image_description)\n",
    "        #there are multiple descriptions per image, hence, corresponding to every image identifier in the\n",
    "        #dictionary, there is a list of description\n",
    "        #if the list does not exist then we need to create it\n",
    "        if image_id not in description_mapping:\n",
    "            description_mapping[image_id] = list()\n",
    "        #now storing the descriptions in the mapping\n",
    "        description_mapping[image_id].append(image_description)\n",
    "    \n",
    "    return description_mapping\n",
    "\n",
    "#function to clean the descriptions in the following ways:\n",
    "#     Convert all words to lowercase.\n",
    "#     Remove all punctuation.\n",
    "#     Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "#     Remove all words with numbers in them.\n",
    "def clean_descriptions(description_mapping):\n",
    "    #making a translation table for removing all the punctuation\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    #traversing through the mapping we created\n",
    "    for key, descriptions in description_mapping.items():\n",
    "        for i in range(len(descriptions)):\n",
    "            description = descriptions[i]\n",
    "            description = description.split()\n",
    "            #converting all the words to lower case\n",
    "            description = [word.lower() for word in description]\n",
    "            #removing the punctuation using the translation table we made\n",
    "            description = [word.translate(table) for word in description]\n",
    "            #removing the words with length =1\n",
    "            description = [word for word in description if len(word)>1]\n",
    "            #removing all words with number in them\n",
    "            description = [word for word in description if word.isalpha()]\n",
    "            #converting the description back to string and overwriting in the descriptions list\n",
    "            descriptions[i] = ' '.join(description)\n",
    "\n",
    "#converting the loaded descriptions into a vocabulary of words\n",
    "# Ideally, we want a vocabulary that is both expressive and as small as possible.\n",
    "# A smaller vocabulary will result in a smaller model that will train faster.\n",
    "# For reference, we can transform the clean descriptions into a set \n",
    "# and print its size to get an idea of the size of our dataset vocabulary.\n",
    "def vocabulary(descriptions):\n",
    "    #creating a set of all descriptions\n",
    "    #sets are highly optimized, don't contain any duplicate values\n",
    "    #there implementation is based on hash table\n",
    "    #hence we get a vocabulary that is both expressive and small\n",
    "    all_descriptions = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_descriptions.update(desc.split()) for desc in descriptions[key]]\n",
    "    \n",
    "    return all_descriptions\n",
    "\n",
    "#function to save the cleaned descriptions into a file one per line\n",
    "def save_cleaned_descriptions(cleaned_descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded descriptions:  8092\n",
      "Vocabulary Size:  8763\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "description_file = load_description_file(filename)\n",
    "descriptions = photo_to_description_mapping(description_file)\n",
    "print('Loaded descriptions: ', len(descriptions))\n",
    "clean_descriptions(descriptions)\n",
    "vocabulary = vocabulary(descriptions)\n",
    "print('Vocabulary Size: ', len(vocabulary))\n",
    "save_cleaned_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general function for loading a file into memeory and return text from it\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#function for loading a predefined list of photo identifiers\n",
    "def load_photo_identifiers(filename):\n",
    "    #loading the file containing the list of photo identifier\n",
    "    file = load_file(filename)\n",
    "    #creating a list for stiring the identifiers\n",
    "    photos = list()\n",
    "    #traversing the file one line at a time\n",
    "    for line in file.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        #image name contains the extension as well. but we just the name\n",
    "        identifier = line.split('.')[0]\n",
    "        #adding it to the list of photos\n",
    "        photos.append(identifier)\n",
    "    #returning the set of photos created\n",
    "    return set(photos)\n",
    "\n",
    "\n",
    "#loading the cleaned descriptions that we created earlier\n",
    "#we will only be loading the descriptions of the images that we will use for training\n",
    "#hence we need to pass the set of train photos that the above function will be returning\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    #loading the cleaned description file\n",
    "    file = load_file(filename)\n",
    "    #creating a dictionary of descripitions for storing the photo to description mapping of train images\n",
    "    descriptions = dict()\n",
    "    #traversing the file line by line\n",
    "    for line in file.split('\\n'):\n",
    "        #splitting the line at white spaces\n",
    "        words = line.split()\n",
    "        #the first word will be the image name and the rest will be the description of that particular image\n",
    "        image_id, image_description = words[0], words[1:]\n",
    "        #we want to load only those description which corresponds to the set of photos we provided as argument\n",
    "        if image_id in photos:\n",
    "            #creating list of description if needed\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            #the model we will develop will generate a caption given a photo, \n",
    "            #and the caption will be generated one word at a time. \n",
    "            #The sequence of previously generated words will be provided as input. \n",
    "            #Therefore, we will need a ‘first word’ to kick-off the generation process \n",
    "            #and a ‘last word‘ to signal the end of the caption.\n",
    "            #we will use 'startseq' and 'endseq' for this purpose\n",
    "            #also we have to conver image description back to string\n",
    "            desc = 'startseq ' + ' '.join(image_description) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "            \n",
    "    return descriptions\n",
    "\n",
    "#function to load the photo features created using the VGG16 model\n",
    "def load_photo_features(filename, photos):\n",
    "    #this will load the entire features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    #we are interested in loading the features of the required photos only\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_photo_identifiers(filename)\n",
    "print('Dataset: ',len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=', len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The description text will need to be encoded to numbers before it can be presented to the model as in input or compared to the model’s predictions.\n",
    "\n",
    "### The first step in encoding the data is to create a consistent mapping from words to unique integer values. Keras provides the Tokenizer class that can learn this mapping from the loaded description data.\n",
    "\n",
    "### Below defines the to_lines() to convert the dictionary of descriptions into a list of strings and the create_tokenizer() function that will fit a Tokenizer given the loaded photo description text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "#given the descriptions, fit a tokenizer\n",
    "\n",
    "#TOKENIZER CLASS:\n",
    "#This class allows to vectorize a text corpus, \n",
    "#by turning each text into either a sequence of integers \n",
    "#(each integer being the index of a token in a dictionary) \n",
    "#or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  7579\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now encode the text.\n",
    "\n",
    "Each description will be split into words. The model will be provided one word and the photo and generate the next word. Then the first two words of the description will be provided to the model as input with the image to generate the next word. This is how the model will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function below, given the tokenizer, max length of sequence, dictionary of descriptions and photo features,\n",
    "#will transform data in input-output pairs of data for training the model. There are two input arrays to the model,\n",
    "#one for the encoded text and one for the photo features. There is one output for the model i.e.\n",
    "#the encoded next word\n",
    "#The output will therefore be a one hot encoded version of each word reperesenting an idealized probability\n",
    "#distribution with 0 value at all word positions except the actual word position which is represented by 1.\n",
    "# def create_sequence(tokenizer, max_length, descriptions, photos):\n",
    "#     x1, x2, y = list(), list(), list()\n",
    "#     for key, description_list in descriptions.items():\n",
    "#         for description in description_list:\n",
    "#             sequence = tokenizer.text_to_sequences([description])[0]\n",
    "#             for i in range(1, len(sequence)):\n",
    "#                 input_sequence, output_sequence = sequence[:i], sequence[i]\n",
    "#                 #pad_sequences function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array\n",
    "#                 #of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, \n",
    "#                 #or the length of the longest sequence otherwise.\n",
    "#                 input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n",
    "#                 #to_categorical function transforms a class vector (integers) to binary class matrix.\n",
    "#                 output_sequence = to_categorical([output_sequence], num_classes=vocab_size)[0]\n",
    "#                 x1.append(photos[key][0])\n",
    "#                 x2.append(input_sequence)\n",
    "#                 y.append(output_sequence)\n",
    "#     return array(x1), array(x2), array(y)\n",
    "\n",
    "#calculated the length of description with most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is in three parts:\n",
    "\n",
    "1. Photo Feature Extractor. This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input.\n",
    "2. Sequence Processor. This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer.\n",
    "3. Decoder. Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Photo Feature Extractor model expects input photo features to be a vector of 4,096 elements. These are processed by a Dense layer to produce a 256 element representation of the photo.\n",
    "\n",
    "The Sequence Processor model expects input sequences with a pre-defined length (34 words) which are fed into an Embedding layer that uses a mask to ignore padded values. This is followed by an LSTM layer with 256 memory units.\n",
    "\n",
    "Both the input models produce a 256 element vector. Further, both input models use regularization in the form of 50% dropout. This is to reduce overfitting the training dataset, as this model configuration learns very fast.\n",
    "\n",
    "The Decoder model merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then to a final output Dense layer that makes a softmax prediction over the entire output vocabulary for the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with progressive loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progressive Loading\n",
    "The Flicr8K dataset of photos and descriptions can fit into RAM, if you have a lot of RAM (e.g. 8 Gigabytes or more), and most modern systems do.\n",
    "\n",
    "This is fine if you want to fit a deep learning model using the CPU.\n",
    "\n",
    "Alternately, if you want to fit a model using a GPU, then you will not be able to fit the data into memory of an average GPU video card.\n",
    "\n",
    "One solution is to progressively load the photos and descriptions as-needed by the model.\n",
    "\n",
    "Keras supports progressively loaded datasets by using the fit_generator() function on the model. A generator is the term used to describe a function used to return batches of samples for the model to train on. This can be as simple as a standalone function, the name of which is passed to the fit_generator() function when fitting the model.\n",
    "\n",
    "As a reminder, a model is fit for multiple epochs, where one epoch is one pass through the entire training dataset, such as all photos. One epoch is comprised of multiple batches of examples where the model weights are updated at the end of each batch.\n",
    "\n",
    "A generator must create and yield one batch of examples. For example, the average sentence length in the dataset is 11 words; that means that each photo will result in 11 examples for fitting the model and two photos will result in about 22 examples on average. A good default batch size for modern hardware may be 32 examples, so that is about 2-3 photos worth of examples.\n",
    "\n",
    "We can write a custom generator to load a few photos and return the samples as a single batch.\n",
    "\n",
    "Let’s assume we are working with a word-by-word model described in the previous section that expects a sequence of words and a prepared image as input and predicts a single word.\n",
    "\n",
    "Let’s design a data generator that given a loaded dictionary of image identifiers to clean descriptions, a trained tokenizer, and a maximum sequence length will load one-image worth of examples for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the below function loop forever with a while loop and within this, \n",
    "#loop over each image in the image directory. \n",
    "#For each image filename, we can load the image and \n",
    "#create all of the input-output sequence pairs from the image’s description.\n",
    "\n",
    "#data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "            #retrieve photo features\n",
    "            photo = photos[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequence(tokenizer, max_length, description_lsit, photo)\n",
    "            yield [[input_image, input_sequence], output_word]\n",
    "\n",
    "            \n",
    "#we are calling the create_sequence() function to create \n",
    "#a batch worth of data for a single photo rather than an entire dataset. \n",
    "#This means that we must update the create_sequences() function \n",
    "#to delete the “iterate over all descriptions” for-loop.            \n",
    "#Updated create sequence function for data_generator\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        sequence = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(sequence)):\n",
    "            input_sequence, output_sequence = sequence[:i], sequence[i]\n",
    "            #pad_sequences function transforms a list of num_samples sequences (lists of integers) into a 2D Numpy array\n",
    "            #of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, \n",
    "            #or the length of the longest sequence otherwise.\n",
    "            input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n",
    "            #to_categorical function transforms a class vector (integers) to binary class matrix.\n",
    "            output_sequence = to_categorical([output_sequence], num_classes=vocab_size)[0]\n",
    "            x1.append(photo)\n",
    "            x2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(x1), array(x2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length:  34\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'description_lsit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9acfe483108f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#     model.save('model_' + str(i) + '.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-352e2d939e34>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(descriptions, photos, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m#retrieve photo features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mphoto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphotos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription_lsit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'description_lsit' is not defined"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_photo_identifiers(filename)\n",
    "print('Dataset: ', len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=', len(train_features))\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function maps an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "#The function below generates a textual description given a trained model, \n",
    "#and a given prepared photo as input. It calls the function word_for_id() \n",
    "#in order to map an integer prediction back to a word.\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    #start tge generation process\n",
    "    in_text = 'startseq'\n",
    "    #iterating over the max_length since the maximum length of the description can be that only\n",
    "    for i in range(max_length):\n",
    "        #integer ncoding input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #padding the input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        #predicting next word\n",
    "        #the predict function will return probability\n",
    "        prob = model.predict([photo,sequence], verbose=0)\n",
    "        #converting the probability to integer\n",
    "        prob = argmax(prob)\n",
    "        #calling the word_for_id function in order to map integer to word\n",
    "        word = word_for_id(prob, tokenizer)\n",
    "        #breaking if word cannot be mapped\n",
    "        if word is None:\n",
    "            break\n",
    "        #appending as input\n",
    "        in_text += ' ' + word\n",
    "        #break if end is predicted\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "#the below function evaluates the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        prediction = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        actual_desc = [d.split() for d in desc_list]\n",
    "        actual.append(actual_desc)\n",
    "        predicted.append(prediction.split())\n",
    "\n",
    "    print('BLEU-1: ', corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: ', corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: ', corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: ', corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Vocabulary Size:  7579\n",
      "Description Length: , 34\n",
      "Dataset:  1000\n",
      "Descriptions: test= 1000\n",
      "Photos: test= 1000\n",
      "BLEU-1:  0.5079996300749098\n",
      "BLEU-2:  0.26815254395800053\n",
      "BLEU-3:  0.17788590578166746\n",
      "BLEU-4:  0.07615229858117227\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_photo_identifiers(filename)\n",
    "print('Dataset: ', len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: ', vocab_size)\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: ,', max_length)\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_photo_identifiers(filename)\n",
    "print('Dataset: ', len(test))\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=', len(test_descriptions))\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=', len(test_features))\n",
    "\n",
    "filename = 'model_12.h5'\n",
    "model = load_model(filename)\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate New Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_photo_identifiers(filename)\n",
    "print('Dataset: ', len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filename):\n",
    "    model = VGG16()\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "max_length = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq man in black shirt is walking on the street endseq\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model_13.h5')\n",
    "photo = extract_features('example.jpg')\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
